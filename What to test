Some models do not accept batch size of one, maybe because it is using batch normalization. One workaround is to use group normalization (GN), but there are models such as segformer that acceps batch size of one, even with batch normalization. Maybe it depends on 'where' in the model the batch normalization is used (like the backbone or the head).

In 'test.py', there is an argument 'local rank', what is it ? - (answer from copilot:) It is the rank of the gpu in the node. It is used to set the device of the model. It is set by the 'torch.distributed.launch' command.

I think I have seen some 'resize' in the traain pipeline that have a parameter 'divisor' (or sg like that) that is set to 32. What is it ? Maybe it is such that the size the image will be resized to is a multiple of 32 ? May
be usefull.

Remove pretrained weights - Maybe not, bc it is (maybe) not the weights after training fully on for example cityscapes

change the 'mean' and 'std' values in the data preprocessor

pidnet

test loop, or the 'test.py' file to test the model ?

RandomResize with crop versus just a Resize

the val and test dataloader with a batch size > 1 - From bard, it seems that the val and test dataloader should have a batch size of 1. But from testing it seems that it is not the case. A batch size of 2 is ok.

The performance in fp32 and fp16 (with and without amp) of every gpu in the cluster - In fp16 : tesla>a5000>quadro>2080ti, but preferably use a5000 and quadro since there are more of them in the cluster.

the hidden index or remove index in dataset configs

The customized loss per class

The vizualization hook

What is tta, or test time augmentation ? Checkout how to make a tta model by viewing differences between tta and non tta testing in the test reps.

In the transform pipelines, what is the infiniteSampler ?

randomness - Normally setting 'randomness = dict(seed=seed, deterministic=True)' in the cfg should work, but got : RuntimeError: Deterministic behavior was enabled with either `torch.use_deterministic_algorithms(True)` or `at::Context::setDeterministicAlgorithms(true)`, but this operation is not deterministic because it uses CuBLAS and you have CUDA >= 10.2. To enable deterministic behavior in this case, you must set an environment variable before running your PyTorch application: CUBLAS_WORKSPACE_CONFIG=:4096:8 or CUBLAS_WORKSPACE_CONFIG=:16:8. For more information, go to https://docs.nvidia.com/cuda/cublas/index.html#cublasApi_reproducibility

gpus memory size - To confirm, but it seems that 2080ti has 11GB~12GB, a5000 and quadro have 24GB, and tesla has more than 24GB, maybe 48GB or 32GB.

Attention! When a model stops training (by accident or not), the last checkpoint might not have gone through validation. Thus the best thing is probably to use the second last checkpoint.
