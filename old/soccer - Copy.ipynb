{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/open-mmlab/mmsegmentation/blob/main/demo/MMSegmentation_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ta51clKX4cwM"
   },
   "source": [
    "## Finetune a semantic segmentation model on a new dataset\n",
    "\n",
    "To finetune on a customized dataset, the following steps are necessary. \n",
    "1. Add a new dataset class. \n",
    "2. Create a config file accordingly. \n",
    "3. Perform training and evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5mNQuc2GsVE"
   },
   "source": [
    "We need to convert the annotation into semantic map format as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, mmseg, mmcv, mmengine, os, shutil, numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WnGZfribFHCx"
   },
   "outputs": [],
   "source": [
    "# define dataset root and directory for images and annotations\n",
    "data_root = 'Dataset'\n",
    "img_dir = 'Images'\n",
    "ann_dir = 'Labels'\n",
    "\n",
    "# Color code of the annotation images\n",
    "#                              R-G-B\n",
    "# ---------------------------------------------------------\n",
    "# Outside billboards\n",
    "\n",
    "#     0.                    000-000-000  (black)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Inside billboards\n",
    "\n",
    "#     1. Billboard          255-255-255  (white)\n",
    "\n",
    "#     2. Field player       255-000-000  (red)\n",
    "#     3. Goalkeeper         000-255-000  (green)\n",
    "#     4. Referee            000-000-255  (blue)\n",
    "#     5. Assistant referee  255-255-000  (yellow)\n",
    "#     6. Other human        255-000-255  (pink)\n",
    "\n",
    "#     7. Ball               000-255-255  (turquoise)\n",
    "\n",
    "#     8. Goal post          128-000-000  (dark red)\n",
    "#     9. Goal net           000-128-000  (dark green)\n",
    "#    10. Net post           000-000-128  (dark blue)\n",
    "#    11. Cross-bar          064-064-064  (dark gray)\n",
    "\n",
    "#    12. Corner flag        128-128-000  (dark yellow)\n",
    "#    13. Assistant flag     128-000-128  (purple)\n",
    "\n",
    "#    14. Microphone         000-128-128  (dark turquoise)\n",
    "#    15. Camera             255-128-000  (orange)\n",
    "   \n",
    "#    16. Other object       192-192-192  (light gray)\n",
    "  \n",
    "#    17. Don't care         128-128-128  (gray)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Define the classes and the palette for the segmentation of my dataset\n",
    "old_classes = ['Outside billboards', 'Billboard', 'Field player', 'Goalkeeper', 'Referee', 'Assistant referee', 'Other human', 'Ball', 'Goal post', 'Goal net', 'Net post', 'Cross-bar', 'Corner flag', 'Assistant flag', 'Microphone', 'Camera', 'Other object', 'Don\\'t care']\n",
    "old_palette = [[0, 0, 0], [255, 255, 255], [255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0], [255, 0, 255], [0, 255, 255], [128, 0, 0], [0, 128, 0], [0, 0, 128], [64, 64, 64], [128, 128, 0], [128, 0, 128], [0, 128, 128], [255, 128, 0], [192, 192, 192], [128, 128, 128]]\n",
    "old_palette_dict = {tuple(p): i for i, p in enumerate(old_palette)}\n",
    "old_classes_dict = {i: c for i, c in enumerate(old_classes)}\n",
    "\n",
    "new_classes = ['Outside billboards', 'Billboard', 'Goal net']\n",
    "# new_classes = old_classes\n",
    "# new_classes = ['Outside billboards', 'Goal net']\n",
    "\n",
    "new_palette = [old_palette[old_classes.index(c)] for c in new_classes]\n",
    "new_palette_dict = {tuple(p): i for i, p in enumerate(new_palette)}\n",
    "new_classes_dict = {i: c for i, c in enumerate(new_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_old_to_new = np.zeros(len(old_classes), dtype=np.uint8)  # By default, all classes are mapped to 0 (Outside billboards)\n",
    "# conversion_old_to_new = np.array([2 for i in range(len(old_classes))], dtype=np.uint8)  # By default, all classes are mapped to 2 (Other object)\n",
    "conversion_old_to_new[old_classes.index('Billboard')] = new_classes.index('Billboard')\n",
    "conversion_old_to_new[old_classes.index('Goal net')] = new_classes.index('Goal net')\n",
    "\n",
    "# conversion_old_to_new = np.array([i for i in range(len(old_classes))], dtype=np.uint8)  # No conversion\n",
    "\n",
    "# conversion_old_to_new[old_classes.index('Goal net')] = 1  # Goal net\n",
    "print(conversion_old_to_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_classes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_palette_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_palette_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the annotations to the new classes\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "ann_source_dir = 'Labels_source'\n",
    "\n",
    "ann_path = os.path.join(data_root, ann_source_dir, 'Stadium_6_Match_1_in_1fps_6275.png')\n",
    "img = Image.open(ann_path)\n",
    "plt.imshow(np.array(img.convert('RGB')))\n",
    "\n",
    "print(img.size)\n",
    "img_np = np.array(img, dtype=np.uint8)\n",
    "print(img_np.shape)\n",
    "# For each pixel x, replace it by the value conversion_old_to_new[x]\n",
    "img_np = conversion_old_to_new[img_np]\n",
    "img = Image.fromarray(img_np).convert('P')\n",
    "img.putpalette(np.array(new_palette, dtype=np.uint8))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "im = plt.imshow(np.array(img.convert('RGB')))\n",
    "\n",
    "patches = [mpatches.Patch(color=np.array(new_palette[i])/255., label=new_classes[i]) for i in range(len(new_classes))]\n",
    "plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., fontsize='large')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ann_files = os.listdir(os.path.join(data_root, ann_source_dir))\n",
    "\n",
    "def convert_ann_file(files):\n",
    "    for file in tqdm(files):\n",
    "        # Open the image\n",
    "        ann = Image.open(os.path.join(data_root, ann_source_dir, file))\n",
    "        # Create a numpy array from the image\n",
    "        ann_np = np.array(ann, dtype=np.uint8)\n",
    "        # For each pixel x, replace it by the value conversion_old_to_new[x]\n",
    "        ann_np = conversion_old_to_new[ann_np]\n",
    "        # Create the image from the numpy array\n",
    "        seg_img = Image.fromarray(ann_np).convert('P')\n",
    "        # Add the new palette to the image\n",
    "        seg_img.putpalette(np.array(new_palette, dtype=np.uint8))\n",
    "        # Save the image\n",
    "        seg_img.save(os.path.join(data_root, ann_dir, file))\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Define the number of processes\n",
    "num_processes = 16\n",
    "\n",
    "# Create a pool of processes\n",
    "pool = Pool(processes=num_processes)\n",
    "\n",
    "# Split the list of files into 8 chunks\n",
    "files_chunks = np.array_split(all_ann_files, num_processes)\n",
    "\n",
    "# Apply the function convert_ann_file to each chunk\n",
    "pool.map(convert_ann_file, files_chunks)\n",
    "\n",
    "# Close the pool\n",
    "pool.close()\n",
    "\n",
    "# Merge the chunks\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HchvmGYB_rrO"
   },
   "source": [
    "After processing the data, we need to implement `load_annotations` function in the new dataset class `StanfordBackgroundDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbsWOw62_o-X"
   },
   "outputs": [],
   "source": [
    "from mmseg.registry import DATASETS\n",
    "from mmseg.datasets import BaseSegDataset\n",
    "\n",
    "\n",
    "# Check if the dataset SoccerNet is already registered\n",
    "if 'SoccerNet' not in DATASETS:\n",
    "\t@DATASETS.register_module()\n",
    "\tclass SoccerNet(BaseSegDataset):\n",
    "\t\tMETAINFO = dict(classes = new_classes, palette = new_palette)\n",
    "\t\tdef __init__(self, **kwargs):\n",
    "\t\t\tsuper().__init__(img_suffix='.png', seg_map_suffix='.png', **kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUVtmn3Iq3WA"
   },
   "source": [
    "### Create a config file\n",
    "In the next step, we need to modify the config for the training. To accelerate the process, we finetune the model from trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wwnj9tRzqX_A"
   },
   "outputs": [],
   "source": [
    "from mmengine import Config\n",
    "cfg = Config.fromfile('../configs/pspnet/pspnet_r50-d8_4xb2-40k_cityscapes-512x1024.py')\n",
    "# print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y2oV5w97jQo"
   },
   "source": [
    "Since the given config is used to train PSPNet on the cityscapes dataset, we need to modify it accordingly for our new dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyKnYC1Z7iCV",
    "outputId": "6195217b-187f-4675-994b-ba90d8bb3078"
   },
   "outputs": [],
   "source": [
    "# Since we use only one GPU, BN is used instead of SyncBN\n",
    "cfg.norm_cfg = dict(type='BN', requires_grad=True)\n",
    "# cfg.crop_size = (256, 256)\n",
    "cfg.model.data_preprocessor.size = cfg.crop_size\n",
    "cfg.model.backbone.norm_cfg = cfg.norm_cfg\n",
    "cfg.model.decode_head.norm_cfg = cfg.norm_cfg\n",
    "cfg.model.auxiliary_head.norm_cfg = cfg.norm_cfg\n",
    "# modify num classes of the model in decode/auxiliary head\n",
    "cfg.model.decode_head.num_classes = len(new_classes)\n",
    "cfg.model.auxiliary_head.num_classes = len(new_classes)\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.dataset_type = 'SoccerNet'\n",
    "cfg.data_root = data_root\n",
    "\n",
    "cfg.train_dataloader.batch_size = 2\n",
    "\n",
    "cfg.train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations'),\n",
    "    # dict(type='RandomResize', scale=(320, 240), ratio_range=(0.5, 2.0), keep_ratio=True),\n",
    "    # dict(type='Resize', scale=(320, 240), keep_ratio=True),\n",
    "    # dict(type='RandomCrop', crop_size=cfg.crop_size, cat_max_ratio=0.75),\n",
    "    dict(type='RandomFlip', prob=0.5),\n",
    "    dict(type='PackSegInputs')\n",
    "]\n",
    "\n",
    "cfg.test_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    # dict(type='Resize', scale=(320, 240), keep_ratio=True),\n",
    "    # add loading annotation after ``Resize`` because ground truth\n",
    "    # does not need to do resize data transform\n",
    "    dict(type='LoadAnnotations'),\n",
    "    dict(type='PackSegInputs')\n",
    "]\n",
    "\n",
    "\n",
    "cfg.train_dataloader.dataset.type = cfg.dataset_type\n",
    "cfg.train_dataloader.dataset.data_root = cfg.data_root\n",
    "cfg.train_dataloader.dataset.data_prefix = dict(img_path=img_dir, seg_map_path=ann_dir)\n",
    "cfg.train_dataloader.dataset.pipeline = cfg.train_pipeline\n",
    "cfg.train_dataloader.dataset.ann_file = 'splits/train.txt'\n",
    "\n",
    "cfg.val_dataloader.dataset.type = cfg.dataset_type\n",
    "cfg.val_dataloader.dataset.data_root = cfg.data_root\n",
    "cfg.val_dataloader.dataset.data_prefix = dict(img_path=img_dir, seg_map_path=ann_dir)\n",
    "cfg.val_dataloader.dataset.pipeline = cfg.test_pipeline\n",
    "cfg.val_dataloader.dataset.ann_file = 'splits/val.txt'\n",
    "\n",
    "cfg.test_dataloader = cfg.val_dataloader\n",
    "\n",
    "\n",
    "# Load the pretrained weights\n",
    "cfg.load_from = 'pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './work_dirs/tutorial'\n",
    "\n",
    "cfg.train_cfg.max_iters = 2000\n",
    "cfg.train_cfg.val_interval = 200\n",
    "cfg.default_hooks.logger.interval = 10\n",
    "cfg.default_hooks.checkpoint.interval = 200\n",
    "\n",
    "# Set seed to facilitate reproducing the result\n",
    "cfg['randomness'] = dict(seed=0)\n",
    "\n",
    "# Let's have a look at the final config used for training\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWuH14LYF2gQ"
   },
   "source": [
    "### Train and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYKoSfdMF12B",
    "outputId": "422219ca-d7a5-4890-f09f-88c959942e64"
   },
   "outputs": [],
   "source": [
    "from mmengine.runner import Runner\n",
    "\n",
    "runner = Runner.from_cfg(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "runner.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEkWOP-NMbc_"
   },
   "source": [
    "Inference with trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "ekG__UfaH_OU",
    "outputId": "1437419c-869a-4902-df86-d4f6f8b2597a"
   },
   "outputs": [],
   "source": [
    "from mmseg.apis import init_model, inference_model, show_result_pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Init the model from the config and the checkpoint\n",
    "checkpoint_path = './work_dirs/tutorial/iter_2000.pth'\n",
    "model = init_model(cfg, checkpoint_path, 'cuda:0')\n",
    "\n",
    "img = mmcv.imread(os.path.join(data_root, img_dir, 'Stadium_1_Match_1_in_1fps_0491.png'))\n",
    "result = inference_model(model, img)\n",
    "# Show the result without using show_result_pyplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "vis_result = show_result_pyplot(model, img, result)\n",
    "plt.imshow(mmcv.bgr2rgb(vis_result))\n",
    "# Save the result image\n",
    "plt.savefig('result.png')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MMSegmentation Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pt1.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "0442e67aee3d9cbb788fa6e86d60c4ffa94ad7f1943c65abfecb99a6f4696c58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
