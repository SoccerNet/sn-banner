# Banner replacement

<div align="center">

<!-- [![Scholar](https://img.shields.io/badge/Scholar-NBJW-F2F2F2.svg?style=for-the-badge)](https://scholar.google.com/citations?view_op=view_citation&hl=en&user=L-ucdXsAAAAJ&citation_for_view=L-ucdXsAAAAJ:d1gkVwhDpl0C)
[![Conference](https://img.shields.io/badge/CVPRW-2024-6b8bc7.svg?style=for-the-badge)](https://openaccess.thecvf.com/content/CVPR2024W/CVsports/html/Gutierrez-Perez_No_Bells_Just_Whistles_Sports_Field_Registration_by_Leveraging_Geometric_CVPRW_2024_paper.html)
[![arXiv](https://img.shields.io/badge/arXiv-2207.11709-b31b1b.svg?style=for-the-badge)](https://arxiv.org/abs/2404.08401) -->

<!-- Add image ! -->
<!-- <p align="center"><img src=figures/FieldReconstruction-1.png  width=70% height=70%></p> -->

</div>

<!-- ## Contents
- [Abstract](#abstract)
- [Model Weights](#weights)
- [Inference / Demo](#inference)
- [Experiments](#experiments)
    - [Datasets](#datasets)
    - [Metrics](#metrics)
    - [Evaluation](#evaluation)
- [Requirements](#requirements)
- [Citation](#citation) -->

<!-- <hr>   -->


<!-- ## Abstract
Broadcast sports field registration is traditionally addressed as a homography estimation task, mapping the visible image area to a planar field model, predominantly focusing on the main camera shot. Addressing the shortcomings of previous approaches, we propose a novel calibration pipeline enabling camera calibration using a 3D soccer field model and extending the process to assess the multiple-view nature of broadcast videos. Our approach begins with a keypoint generation pipeline derived from SoccerNet dataset annotations, leveraging the geometric properties of the court. Subsequently, we execute classical camera calibration through DLT algorithm in a minimalist fashion, without further refinement. Through extensive experimentation on real-world soccer broadcast datasets such as SoccerNet-Calibration, WorldCup 2014 and TS-WorldCup, our method demonstrates superior performance in both multiple- and single-view 3D camera calibration while maintaining competitive results in homography estimation compared to state-of-the-art techniques
<p align="center"><img src=figures/Pipeline_r.png  width=90% height=90%></p>
<strong>Overview of our proposed framework. Top: Training data generation pipeline. Beginning with SoccerNet annotations,
we utilize field line extraction and ellipse fitting to establish a hierarchical structure for computing each set of keypoints. Bottom: The
encoder-decoder networks produce heatmaps for keypoints and extremities of soccer field lines to extract their positions in the image space.
The obtained keypoint set is augmented with intersections of lines generated by the second model to ensure a sufficient number of points.</strong> -->

<hr>  

## Weights
Download the keypoints and line detection model weights for No-Bells-Just-Whistles camera calibration model, and the mask2former semantic segmentation model weights. Place them in the specified directories.

| Model | Link | Destination |
| :-- | :-: | :-- |
|Keypoints| [**SV_kp**](https://github.com/mguti97/No-Bells-Just-Whistles/releases/download/v1.0.0/SV_kp) | camera_calibration/No-Bells-Just-Whistles/SV_kp |
|Lines| [**SV_lines**](https://github.com/mguti97/No-Bells-Just-Whistles/releases/download/v1.0.0/SV_lines) | camera_calibration/No-Bells-Just-Whistles/SV_lines |
|challenge_mask2former| [**best_mIoU_iter_10935.pth**](https://we.tl/t-OPluzixvT9) | semantic_segmentation/models/challenge_mask2former/best_mIoU_iter_10935.pth |



<hr>

## Installation
<!-- ### [Conda Environment](https://docs.conda.io/en/latest/)
```shell
conda env create -f NBJWCalib.yml
conda activate NBJWCalib
```

### PIP
```shell
python -m venv env
source env/bin/activate
pip install -r requirements.txt
``` -->

Two independent conda environment are needed: One for MMsegmentation and another for the rest of the project. 

### mmseg

The conda environment for MMsegmentation can be created by running the following command:
```shell
conda create -n mmseg python=3.11 -y
conda activate mmseg
```
Install pytorch. The package mmcv, a requirement for mmsegmentation, depends heavily on the pytorch version installed. The recommended version is 2.1.2, with CUDA 11.8 or 12.1. If you want to install another pytorch version, see below. The installation commands for each cuda version and using either pip or conda can be found [here](https://pytorch.org/get-started/previous-versions/#v212). For example, to install the recommended pytorch version with conda and CUDA 11.8:
```shell
conda install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=11.8 -c pytorch -c nvidia
```

If the installation of mmcv fails after the installation of pytorch with conda, try using pip instead, and vice versa.

Then, install [mim](https://github.com/open-mmlab/mim) v0.3.9, a package management tool for the OpenMMLab projects, which makes it easy to install mmcv:
```shell
pip install openmim==0.3.9
```

Install mmengine v0.10.3, mmdet v3.3.0 and mmcv v2.1.0:
```shell
mim install mmengine==0.10.3 mmdet==3.3.0 mmcv==2.1.0
```

If you want to use another version of pytorch and cuda, alternatives to install mmcv v2.1.0 are:
- Let mim check your system and install, if possible, a pre-built package (faster, no compilation needed, list of systems with pre-built packages available [here](https://mmcv.readthedocs.io/en/latest/get_started/installation.html#install-with-pip)) else it will try to built the package.
- Install with pip a pre-built package. The list of type of system, CUDA version, pytorch version and mmcv version (which in our case must be 2.1.0) is available [here](https://mmcv.readthedocs.io/en/latest/get_started/installation.html#install-with-pip).
- Install from [source](https://mmcv.readthedocs.io/en/latest/get_started/build.html)

More information on the [MMCV official installation guide](https://mmcv.readthedocs.io/en/latest/get_started/installation.html#install-with-pip).

Finally, install the rest of the requirements:
```shell
pip install regex==2024.7.24 ftfy==6.2.0 mmsegmentation==1.2.2
```

### banner-replacement

The conda environment for the rest of the project can be created by running the following commands:
```shell
conda create -n banner-replacement python=3.11 -y
conda activate banner-replacement
```

Then, install pytorch from the [official website](https://pytorch.org/get-started/locally/) or a [previous version](https://pytorch.org/get-started/previous-versions/). No-Bells-Just-Whistles, the camera calibration model, has shown improved accuracy with more recent versions. The recommended and most recent pytorch version tested is 2.4.0.

Finally, install the rest of the requirements:
```shell
pip install scikit-learn==1.5.1 opencv-python==4.10.0.84 tqdm==4.66.4 matplotlib==3.8.4 PyYAML==6.0.1 pandas==2.2.2
```


<hr>

## Inference

On video:
``` shell
python inference.py path/to/video.mp4 path/to/logo.png
```

On image sequence from SN-GameState, or any directory with N images named "%06d.jpg" and beginning with "000001.jpg":
``` shell
python inference.py path/to/SoccerNetGS/test/SNGS-142/img1/ path/to/logo.png --sequence
```

Optional arguments:
- ```--n_workers```: Number of workers to speed up some steps of the pipeline. Default is 1. Attention it may use a lot of memory.
- ```--tta```: Whether to use test-time augmentation. Default is False. A GPU with at least 24GB of memory is required.

<hr>

<!-- ## Experiments
### Datasets
#### SoccerNet-Calibration-V3:
To download the 2022 version, switch the downloader's task to "calibration".
``` shell
from SoccerNet.Downloader import SoccerNetDownloader
mySoccerNetDownloader = SoccerNetDownloader(LocalDirectory="</nfs/data/soccernet>")
mySoccerNetDownloader.downloadDataTask(task="calibration-2023", split=["train","valid","test"])
```
See https://github.com/MM4SPA/tvcalib to download the camera-type annotations for SoccerNet-Calibration-2022.

#### WorldCup 2014
```shell
mkdir -p datasets/WC-2014/test && cd datasets/WC-2014/test
# Images and provided homography matrices from test split
wget https://nhoma.github.io/data/soccer_data.tar.gz
tar -zxvf soccer_data.tar.gz
```

See https://github.com/MM4SPA/tvcalib to download the additional segment annotations in SoccerNet-Calibration format.

#### TS-WorldCup
Download the [TS-WorldCup](https://cgv.cs.nthu.edu.tw/KpSFR_data/TS-WorldCup.zip) dataset

<hr>

### Metrics
#### Segment Reprojection Error
See https://github.com/SoccerNet/sn-calibration for details on the evaluation metric.

#### IoU, Projection error and Reprojection error:
For the Homography Estimation evaluation, we adopt the approach outlined at https://github.com/ericsujw/KpSFR. Minor modifications of the used script can be seen in
```model/metrics.py ```

<hr>

### Evaluation
We provide scripts ```scripts/``` to reproduce the paper's results for the presented approach.
Make sure to change the dataset location on the bash scripts, default is set as ```"datasets/calibration-2023/"```, and the location of model weights, default is set as ```weights/MV_kp```.

```shell
#Multi-view camera parameter estimation for SN-Calib-2023
chmod +x scripts/run_pipeline_sn23.sh
./scripts/run_pipeline_sn23.sh

#Single-view camera parameter estimation for SN-Calib-2022
chmod +x scripts/run_pipeline_sn22.sh
./scripts/run_pipeline_sn22.sh

#WorldCup 2014 as camera parameter estimation task
#Change the dataset location inside "scripts/run_pipeline_sn23.sh" file, i.e. "datasets/WC-2014"
#Change the save directory inside "scripts/run_pipeline_sn23.sh" file to "inference/inference_3D/inference_wc14"
chmod +x scripts/run_pipeline_sn22.sh
./scripts/run_pipeline_sn22.sh

#WorldCup 2014 homography estimation task
chmod +x scripts/run_pipeline_wc14.sh
./scripts/run_pipeline_wc14.sh

#TS-WorldCup homography estimation task
chmod +x scripts/run_pipeline_tswc.sh
./scripts/run_pipeline_tswc.sh
```

Task results will be printed on screen.

<hr>

## Citation
```shell
@inproceedings{gutierrez2024no,
  title={No Bells, Just Whistles: Sports Field Registration by Leveraging Geometric Properties},
  author={Guti{\'e}rrez-P{\'e}rez, Marc and Agudo, Antonio},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3325--3334},
  year={2024}
}
```
 -->
