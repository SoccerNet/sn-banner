{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/open-mmlab/mmsegmentation/blob/main/demo/MMSegmentation_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ta51clKX4cwM"
   },
   "source": [
    "## Finetune a semantic segmentation model on a new dataset\n",
    "\n",
    "To finetune on a customized dataset, the following steps are necessary. \n",
    "1. Add a new dataset class. \n",
    "2. Create a config file accordingly. \n",
    "3. Perform training and evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5mNQuc2GsVE"
   },
   "source": [
    "We need to convert the annotation into semantic map format as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, mmseg, mmcv, mmengine, os, shutil, numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WnGZfribFHCx"
   },
   "outputs": [],
   "source": [
    "# define dataset root and directory for images and annotations\n",
    "data_root = 'Dataset'\n",
    "img_dir = 'Images'\n",
    "ann_dir = 'Labels'\n",
    "\n",
    "# Color code of the annotation images\n",
    "#                              R-G-B\n",
    "# ---------------------------------------------------------\n",
    "# Outside billboards\n",
    "\n",
    "#     0.                    000-000-000  (black)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Inside billboards\n",
    "\n",
    "#     1. Billboard          255-255-255  (white)\n",
    "\n",
    "#     2. Field player       255-000-000  (red)\n",
    "#     3. Goalkeeper         000-255-000  (green)\n",
    "#     4. Referee            000-000-255  (blue)\n",
    "#     5. Assistant referee  255-255-000  (yellow)\n",
    "#     6. Other human        255-000-255  (pink)\n",
    "\n",
    "#     7. Ball               000-255-255  (turquoise)\n",
    "\n",
    "#     8. Goal post          128-000-000  (dark red)\n",
    "#     9. Goal net           000-128-000  (dark green)\n",
    "#    10. Net post           000-000-128  (dark blue)\n",
    "#    11. Cross-bar          064-064-064  (dark gray)\n",
    "\n",
    "#    12. Corner flag        128-128-000  (dark yellow)\n",
    "#    13. Assistant flag     128-000-128  (purple)\n",
    "\n",
    "#    14. Microphone         000-128-128  (dark turquoise)\n",
    "#    15. Camera             255-128-000  (orange)\n",
    "   \n",
    "#    16. Other object       192-192-192  (light gray)\n",
    "  \n",
    "#    17. Don't care         128-128-128  (gray)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Define the classes and the palette for the segmentation of my dataset\n",
    "old_classes = ['Outside billboards', 'Billboard', 'Field player', 'Goalkeeper', 'Referee', 'Assistant referee', 'Other human', 'Ball', 'Goal post', 'Goal net', 'Net post', 'Cross-bar', 'Corner flag', 'Assistant flag', 'Microphone', 'Camera', 'Other object', 'Don\\'t care']\n",
    "old_palette = [[0, 0, 0], [255, 255, 255], [255, 0, 0], [0, 255, 0], [0, 0, 255], [255, 255, 0], [255, 0, 255], [0, 255, 255], [128, 0, 0], [0, 128, 0], [0, 0, 128], [64, 64, 64], [128, 128, 0], [128, 0, 128], [0, 128, 128], [255, 128, 0], [192, 192, 192], [128, 128, 128]]\n",
    "old_palette_dict = {tuple(p): i for i, p in enumerate(old_palette)}\n",
    "old_classes_dict = {i: c for i, c in enumerate(old_classes)}\n",
    "\n",
    "new_classes = ['Outside billboards', 'Billboard', 'Goal net']\n",
    "# new_classes = old_classes\n",
    "# new_classes = ['Outside billboards', 'Goal net']\n",
    "\n",
    "new_palette = [old_palette[old_classes.index(c)] for c in new_classes]\n",
    "new_palette_dict = {tuple(p): i for i, p in enumerate(new_palette)}\n",
    "new_classes_dict = {i: c for i, c in enumerate(new_classes)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_old_to_new = np.zeros(len(old_classes), dtype=np.uint8)  # By default, all classes are mapped to 0 (Outside billboards)\n",
    "# conversion_old_to_new = np.array([2 for i in range(len(old_classes))], dtype=np.uint8)  # By default, all classes are mapped to 2 (Other object)\n",
    "conversion_old_to_new[old_classes.index('Billboard')] = new_classes.index('Billboard')\n",
    "conversion_old_to_new[old_classes.index('Goal net')] = new_classes.index('Goal net')\n",
    "\n",
    "# conversion_old_to_new = np.array([i for i in range(len(old_classes))], dtype=np.uint8)  # No conversion\n",
    "\n",
    "# conversion_old_to_new[old_classes.index('Goal net')] = 1  # Goal net\n",
    "print(conversion_old_to_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_classes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_palette_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_classes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_palette_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_source_dir = 'Labels_source'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the annotations to the new classes\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "ann_source_dir = 'Labels_source'\n",
    "\n",
    "ann_path = os.path.join(data_root, ann_source_dir, 'Stadium_6_Match_1_in_1fps_6275.png')\n",
    "img = Image.open(ann_path)\n",
    "plt.imshow(np.array(img.convert('RGB')))\n",
    "\n",
    "print(img.size)\n",
    "img_np = np.array(img, dtype=np.uint8)\n",
    "print(img_np.shape)\n",
    "# For each pixel x, replace it by the value conversion_old_to_new[x]\n",
    "img_np = conversion_old_to_new[img_np]\n",
    "img = Image.fromarray(img_np).convert('P')\n",
    "img.putpalette(np.array(new_palette, dtype=np.uint8))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "im = plt.imshow(np.array(img.convert('RGB')))\n",
    "\n",
    "patches = [mpatches.Patch(color=np.array(new_palette[i])/255., label=new_classes[i]) for i in range(len(new_classes))]\n",
    "plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0., fontsize='large')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ann_files = os.listdir(os.path.join(data_root, ann_source_dir))\n",
    "\n",
    "def convert_ann_file(files):\n",
    "    for file in tqdm(files):\n",
    "        # Open the image\n",
    "        ann = Image.open(os.path.join(data_root, ann_source_dir, file))\n",
    "        # Create a numpy array from the image\n",
    "        ann_np = np.array(ann, dtype=np.uint8)\n",
    "        # For each pixel x, replace it by the value conversion_old_to_new[x]\n",
    "        ann_np = conversion_old_to_new[ann_np]\n",
    "        # Create the image from the numpy array\n",
    "        seg_img = Image.fromarray(ann_np).convert('P')\n",
    "        # Add the new palette to the image\n",
    "        seg_img.putpalette(np.array(new_palette, dtype=np.uint8))\n",
    "        # Save the image\n",
    "        seg_img.save(os.path.join(data_root, ann_dir, file))\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# Define the number of processes\n",
    "num_processes = 16\n",
    "\n",
    "# Create a pool of processes\n",
    "pool = Pool(processes=num_processes)\n",
    "\n",
    "# Split the list of files into 8 chunks\n",
    "files_chunks = np.array_split(all_ann_files, num_processes)\n",
    "\n",
    "# Apply the function convert_ann_file to each chunk\n",
    "pool.map(convert_ann_file, files_chunks)\n",
    "\n",
    "# Close the pool\n",
    "pool.close()\n",
    "\n",
    "# Merge the chunks\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HchvmGYB_rrO"
   },
   "source": [
    "After processing the data, we need to implement `load_annotations` function in the new dataset class `StanfordBackgroundDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LbsWOw62_o-X"
   },
   "outputs": [],
   "source": [
    "from mmseg.registry import DATASETS\n",
    "from mmseg.datasets import BaseSegDataset\n",
    "\n",
    "\n",
    "# Check if the dataset SoccerNet is already registered\n",
    "if 'SoccerNet' not in DATASETS:\n",
    "\t@DATASETS.register_module()\n",
    "\tclass SoccerNet(BaseSegDataset):\n",
    "\t\tMETAINFO = dict(classes = new_classes, palette = new_palette)\n",
    "\t\tdef __init__(self, **kwargs):\n",
    "\t\t\tsuper().__init__(img_suffix='.png', seg_map_suffix='.png', **kwargs)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUVtmn3Iq3WA"
   },
   "source": [
    "### Create a config file\n",
    "In the next step, we need to modify the config for the training. To accelerate the process, we finetune the model from trained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Wwnj9tRzqX_A"
   },
   "outputs": [],
   "source": [
    "from mmengine import Config\n",
    "cfg = Config.fromfile('pspnet_r50-d8_4xb2-40k_cityscapes-512x1024.py')\n",
    "# print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1y2oV5w97jQo"
   },
   "source": [
    "Since the given config is used to train PSPNet on the cityscapes dataset, we need to modify it accordingly for our new dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyKnYC1Z7iCV",
    "outputId": "6195217b-187f-4675-994b-ba90d8bb3078"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      "crop_size = (\n",
      "    512,\n",
      "    1024,\n",
      ")\n",
      "data_preprocessor = dict(\n",
      "    bgr_to_rgb=True,\n",
      "    mean=[\n",
      "        123.675,\n",
      "        116.28,\n",
      "        103.53,\n",
      "    ],\n",
      "    pad_val=0,\n",
      "    seg_pad_val=255,\n",
      "    size=(\n",
      "        512,\n",
      "        1024,\n",
      "    ),\n",
      "    std=[\n",
      "        58.395,\n",
      "        57.12,\n",
      "        57.375,\n",
      "    ],\n",
      "    type='SegDataPreProcessor')\n",
      "data_root = 'Dataset'\n",
      "dataset_type = 'SoccerNet'\n",
      "default_hooks = dict(\n",
      "    checkpoint=dict(by_epoch=False, interval=200, type='CheckpointHook'),\n",
      "    logger=dict(interval=10, log_metric_by_epoch=False, type='LoggerHook'),\n",
      "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
      "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
      "    timer=dict(type='IterTimerHook'),\n",
      "    visualization=dict(type='SegVisualizationHook'))\n",
      "default_scope = 'mmseg'\n",
      "env_cfg = dict(\n",
      "    cudnn_benchmark=True,\n",
      "    dist_cfg=dict(backend='nccl'),\n",
      "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))\n",
      "img_ratios = [\n",
      "    0.5,\n",
      "    0.75,\n",
      "    1.0,\n",
      "    1.25,\n",
      "    1.5,\n",
      "    1.75,\n",
      "]\n",
      "load_from = None\n",
      "log_level = 'INFO'\n",
      "log_processor = dict(by_epoch=False)\n",
      "model = dict(\n",
      "    auxiliary_head=dict(\n",
      "        align_corners=False,\n",
      "        channels=256,\n",
      "        concat_input=False,\n",
      "        dropout_ratio=0.1,\n",
      "        in_channels=1024,\n",
      "        in_index=2,\n",
      "        loss_decode=dict(\n",
      "            loss_weight=0.4, type='CrossEntropyLoss', use_sigmoid=False),\n",
      "        norm_cfg=dict(requires_grad=True, type='SyncBN'),\n",
      "        num_classes=3,\n",
      "        num_convs=1,\n",
      "        type='FCNHead'),\n",
      "    backbone=dict(\n",
      "        contract_dilation=True,\n",
      "        depth=50,\n",
      "        dilations=(\n",
      "            1,\n",
      "            1,\n",
      "            2,\n",
      "            4,\n",
      "        ),\n",
      "        norm_cfg=dict(requires_grad=True, type='SyncBN'),\n",
      "        norm_eval=False,\n",
      "        num_stages=4,\n",
      "        out_indices=(\n",
      "            0,\n",
      "            1,\n",
      "            2,\n",
      "            3,\n",
      "        ),\n",
      "        strides=(\n",
      "            1,\n",
      "            2,\n",
      "            1,\n",
      "            1,\n",
      "        ),\n",
      "        style='pytorch',\n",
      "        type='ResNetV1c'),\n",
      "    data_preprocessor=dict(\n",
      "        bgr_to_rgb=True,\n",
      "        mean=[\n",
      "            123.675,\n",
      "            116.28,\n",
      "            103.53,\n",
      "        ],\n",
      "        pad_val=0,\n",
      "        seg_pad_val=255,\n",
      "        size=(\n",
      "            512,\n",
      "            1024,\n",
      "        ),\n",
      "        std=[\n",
      "            58.395,\n",
      "            57.12,\n",
      "            57.375,\n",
      "        ],\n",
      "        type='SegDataPreProcessor'),\n",
      "    decode_head=dict(\n",
      "        align_corners=False,\n",
      "        channels=512,\n",
      "        dropout_ratio=0.1,\n",
      "        in_channels=2048,\n",
      "        in_index=3,\n",
      "        loss_decode=dict(\n",
      "            loss_weight=1.0, type='CrossEntropyLoss', use_sigmoid=False),\n",
      "        norm_cfg=dict(requires_grad=True, type='SyncBN'),\n",
      "        num_classes=3,\n",
      "        pool_scales=(\n",
      "            1,\n",
      "            2,\n",
      "            3,\n",
      "            6,\n",
      "        ),\n",
      "        type='PSPHead'),\n",
      "    pretrained='open-mmlab://resnet50_v1c',\n",
      "    test_cfg=dict(mode='whole'),\n",
      "    train_cfg=dict(),\n",
      "    type='EncoderDecoder')\n",
      "norm_cfg = dict(requires_grad=True, type='SyncBN')\n",
      "optim_wrapper = dict(\n",
      "    clip_grad=None,\n",
      "    optimizer=dict(lr=0.01, momentum=0.9, type='SGD', weight_decay=0.0005),\n",
      "    type='OptimWrapper')\n",
      "optimizer = dict(lr=0.01, momentum=0.9, type='SGD', weight_decay=0.0005)\n",
      "param_scheduler = [\n",
      "    dict(\n",
      "        begin=0,\n",
      "        by_epoch=False,\n",
      "        end=40000,\n",
      "        eta_min=0.0001,\n",
      "        power=0.9,\n",
      "        type='PolyLR'),\n",
      "]\n",
      "randomness = dict(seed=0)\n",
      "resume = False\n",
      "test_cfg = dict(type='TestLoop')\n",
      "test_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='splits/val.txt',\n",
      "        data_prefix=dict(img_path='Images', seg_map_path='Labels'),\n",
      "        data_root='Dataset',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations'),\n",
      "            dict(type='PackSegInputs'),\n",
      "        ],\n",
      "        type='SoccerNet'),\n",
      "    num_workers=4,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "test_evaluator = dict(\n",
      "    iou_metrics=[\n",
      "        'mIoU',\n",
      "    ], type='IoUMetric')\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations'),\n",
      "    dict(type='PackSegInputs'),\n",
      "]\n",
      "train_cfg = dict(max_iters=2000, type='IterBasedTrainLoop', val_interval=200)\n",
      "train_dataloader = dict(\n",
      "    batch_size=2,\n",
      "    dataset=dict(\n",
      "        ann_file='splits/train.txt',\n",
      "        data_prefix=dict(img_path='Images', seg_map_path='Labels'),\n",
      "        data_root='Dataset',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations'),\n",
      "            dict(prob=0.5, type='RandomFlip'),\n",
      "            dict(type='PackSegInputs'),\n",
      "        ],\n",
      "        type='SoccerNet'),\n",
      "    num_workers=2,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=True, type='InfiniteSampler'))\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(type='LoadAnnotations'),\n",
      "    dict(prob=0.5, type='RandomFlip'),\n",
      "    dict(type='PackSegInputs'),\n",
      "]\n",
      "tta_model = dict(type='SegTTAModel')\n",
      "tta_pipeline = [\n",
      "    dict(backend_args=None, type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        transforms=[\n",
      "            [\n",
      "                dict(keep_ratio=True, scale_factor=0.5, type='Resize'),\n",
      "                dict(keep_ratio=True, scale_factor=0.75, type='Resize'),\n",
      "                dict(keep_ratio=True, scale_factor=1.0, type='Resize'),\n",
      "                dict(keep_ratio=True, scale_factor=1.25, type='Resize'),\n",
      "                dict(keep_ratio=True, scale_factor=1.5, type='Resize'),\n",
      "                dict(keep_ratio=True, scale_factor=1.75, type='Resize'),\n",
      "            ],\n",
      "            [\n",
      "                dict(direction='horizontal', prob=0.0, type='RandomFlip'),\n",
      "                dict(direction='horizontal', prob=1.0, type='RandomFlip'),\n",
      "            ],\n",
      "            [\n",
      "                dict(type='LoadAnnotations'),\n",
      "            ],\n",
      "            [\n",
      "                dict(type='PackSegInputs'),\n",
      "            ],\n",
      "        ],\n",
      "        type='TestTimeAug'),\n",
      "]\n",
      "val_cfg = dict(type='ValLoop')\n",
      "val_dataloader = dict(\n",
      "    batch_size=1,\n",
      "    dataset=dict(\n",
      "        ann_file='splits/val.txt',\n",
      "        data_prefix=dict(img_path='Images', seg_map_path='Labels'),\n",
      "        data_root='Dataset',\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(type='LoadAnnotations'),\n",
      "            dict(type='PackSegInputs'),\n",
      "        ],\n",
      "        type='SoccerNet'),\n",
      "    num_workers=4,\n",
      "    persistent_workers=True,\n",
      "    sampler=dict(shuffle=False, type='DefaultSampler'))\n",
      "val_evaluator = dict(\n",
      "    iou_metrics=[\n",
      "        'mIoU',\n",
      "    ], type='IoUMetric')\n",
      "vis_backends = [\n",
      "    dict(type='LocalVisBackend'),\n",
      "]\n",
      "visualizer = dict(\n",
      "    name='visualizer',\n",
      "    type='SegLocalVisualizer',\n",
      "    vis_backends=[\n",
      "        dict(type='LocalVisBackend'),\n",
      "    ])\n",
      "work_dir = './work_dirs/tutorial'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Since we use only one GPU, BN is used instead of SyncBN\n",
    "# cfg.norm_cfg = dict(type='BN', requires_grad=True)\n",
    "# cfg.crop_size = (256, 256)\n",
    "cfg.model.data_preprocessor.size = cfg.crop_size\n",
    "cfg.model.backbone.norm_cfg = cfg.norm_cfg\n",
    "cfg.model.decode_head.norm_cfg = cfg.norm_cfg\n",
    "cfg.model.auxiliary_head.norm_cfg = cfg.norm_cfg\n",
    "# modify num classes of the model in decode/auxiliary head\n",
    "cfg.model.decode_head.num_classes = len(new_classes)\n",
    "cfg.model.auxiliary_head.num_classes = len(new_classes)\n",
    "\n",
    "# Modify dataset type and path\n",
    "cfg.dataset_type = 'SoccerNet'\n",
    "cfg.data_root = data_root\n",
    "\n",
    "cfg.train_dataloader.batch_size = 2\n",
    "\n",
    "cfg.train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations'),\n",
    "    # dict(type='RandomResize', scale=(320, 240), ratio_range=(0.5, 2.0), keep_ratio=True),\n",
    "    # dict(type='Resize', scale=(320, 240), keep_ratio=True),\n",
    "    # dict(type='RandomCrop', crop_size=cfg.crop_size, cat_max_ratio=0.75),\n",
    "    dict(type='RandomFlip', prob=0.5),\n",
    "    dict(type='PackSegInputs')\n",
    "]\n",
    "\n",
    "cfg.test_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    # dict(type='Resize', scale=(320, 240), keep_ratio=True),\n",
    "    # add loading annotation after ``Resize`` because ground truth\n",
    "    # does not need to do resize data transform\n",
    "    dict(type='LoadAnnotations'),\n",
    "    dict(type='PackSegInputs')\n",
    "]\n",
    "\n",
    "\n",
    "cfg.train_dataloader.dataset.type = cfg.dataset_type\n",
    "cfg.train_dataloader.dataset.data_root = cfg.data_root\n",
    "cfg.train_dataloader.dataset.data_prefix = dict(img_path=img_dir, seg_map_path=ann_dir)\n",
    "cfg.train_dataloader.dataset.pipeline = cfg.train_pipeline\n",
    "cfg.train_dataloader.dataset.ann_file = 'splits/train.txt'\n",
    "\n",
    "cfg.val_dataloader.dataset.type = cfg.dataset_type\n",
    "cfg.val_dataloader.dataset.data_root = cfg.data_root\n",
    "cfg.val_dataloader.dataset.data_prefix = dict(img_path=img_dir, seg_map_path=ann_dir)\n",
    "cfg.val_dataloader.dataset.pipeline = cfg.test_pipeline\n",
    "cfg.val_dataloader.dataset.ann_file = 'splits/val.txt'\n",
    "\n",
    "cfg.test_dataloader = cfg.val_dataloader\n",
    "\n",
    "\n",
    "# Load the pretrained weights\n",
    "# cfg.load_from = 'pspnet_r50-d8_512x1024_40k_cityscapes_20200605_003338-2966598c.pth'\n",
    "\n",
    "# Set up working dir to save files and logs.\n",
    "cfg.work_dir = './work_dirs/tutorial'\n",
    "\n",
    "cfg.train_cfg.max_iters = 2000\n",
    "cfg.train_cfg.val_interval = 200\n",
    "cfg.default_hooks.logger.interval = 10\n",
    "cfg.default_hooks.checkpoint.interval = 200\n",
    "\n",
    "# Set seed to facilitate reproducing the result\n",
    "cfg['randomness'] = dict(seed=0)\n",
    "\n",
    "# Let's have a look at the final config used for training\n",
    "print(f'Config:\\n{cfg.pretty_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWuH14LYF2gQ"
   },
   "source": [
    "### Train and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYKoSfdMF12B",
    "outputId": "422219ca-d7a5-4890-f09f-88c959942e64"
   },
   "outputs": [],
   "source": [
    "from mmengine.runner import Runner\n",
    "\n",
    "runner = Runner.from_cfg(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "runner.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEkWOP-NMbc_"
   },
   "source": [
    "Inference with trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 645
    },
    "id": "ekG__UfaH_OU",
    "outputId": "1437419c-869a-4902-df86-d4f6f8b2597a"
   },
   "outputs": [],
   "source": [
    "from mmseg.apis import init_model, inference_model, show_result_pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Init the model from the config and the checkpoint\n",
    "checkpoint_path = './work_dirs/tutorial/iter_2000.pth'\n",
    "model = init_model(cfg, checkpoint_path, 'cuda:0')\n",
    "\n",
    "img = mmcv.imread(os.path.join(data_root, img_dir, 'Stadium_1_Match_1_in_1fps_0491.png'))\n",
    "result = inference_model(model, img)\n",
    "# Show the result without using show_result_pyplot\n",
    "plt.figure(figsize=(8, 6))\n",
    "vis_result = show_result_pyplot(model, img, result)\n",
    "plt.imshow(mmcv.bgr2rgb(vis_result))\n",
    "# Save the result image\n",
    "plt.savefig('result.png')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "MMSegmentation Tutorial.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pt1.12')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "vscode": {
   "interpreter": {
    "hash": "0442e67aee3d9cbb788fa6e86d60c4ffa94ad7f1943c65abfecb99a6f4696c58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
